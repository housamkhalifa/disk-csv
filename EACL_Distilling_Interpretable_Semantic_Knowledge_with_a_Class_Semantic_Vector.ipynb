{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "#  Create the multihead\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0XNcpFyXHHIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMRWRWMNDew_",
        "outputId": "0522ab3a-3a1e-4e26-9afa-6e47c915177b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "def layer_normalization(inputs,\n",
        "                        epsilon=1e-8,\n",
        "                        scope=\"ln\",\n",
        "                        reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        inputs_shape = inputs.get_shape()\n",
        "        params_shape = inputs_shape[-1:]\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "        beta = tf.Variable(tf.zeros(params_shape))\n",
        "        gamma = tf.Variable(tf.ones(params_shape))\n",
        "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
        "        outputs = gamma * normalized + beta\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def multihead_attention(queries,\n",
        "                        keys,\n",
        "                        num_units=None,\n",
        "                        num_heads=8,\n",
        "                        dropout_rate=0,\n",
        "                        is_training=True,\n",
        "                        causality=False,\n",
        "                        scope=\"multihead_attention\",\n",
        "                        reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        if num_units is None:  # set default size for attention size C\n",
        "            num_units = queries.get_shape().as_list()[-1]\n",
        "\n",
        "        # Linear Projections\n",
        "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)  # [N, T_q, C]\n",
        "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n",
        "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n",
        "\n",
        "        # Split and concat\n",
        "        Q_ = tf.concat(tf.split(Q, num_heads, axis=-1), axis=0)  # [num_heads * N, T_q, C/num_heads]\n",
        "        K_ = tf.concat(tf.split(K, num_heads, axis=-1), axis=0)  # [num_heads * N, T_k, C/num_heads]\n",
        "        V_ = tf.concat(tf.split(V, num_heads, axis=-1), axis=0)  # [num_heads * N, T_k, C/num_heads]\n",
        "\n",
        "        # Attention\n",
        "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (num_heads * N, T_q, T_k)\n",
        "\n",
        "        # Scale : outputs = outputs / sqrt( d_k)\n",
        "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "        # Key Masking\n",
        "        # see : https://github.com/Kyubyong/transformer/issues/3\n",
        "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n",
        "        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n",
        "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n",
        "\n",
        "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)  # -infinity\n",
        "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Causality = Future blinding\n",
        "        if causality:\n",
        "            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n",
        "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()  # (T_q, T_k)\n",
        "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n",
        "\n",
        "            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
        "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Activation: outputs is a weight matrix\n",
        "        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Query Masking\n",
        "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n",
        "        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n",
        "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n",
        "        outputs *= query_masks  # broadcasting. (N, T_q, C)\n",
        "\n",
        "        # dropouts\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "\n",
        "        # weighted sum\n",
        "        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n",
        "\n",
        "        # reshape\n",
        "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)\n",
        "\n",
        "        # residual connection\n",
        "        outputs += queries\n",
        "\n",
        "        # layer normaliztion\n",
        "        outputs = layer_normalization(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def feedforward(inputs,\n",
        "                num_units=[2048, 512],\n",
        "                scope=\"multihead_attention\",\n",
        "                reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Inner layer\n",
        "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
        "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "\n",
        "        # Readout layer\n",
        "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
        "                  \"activation\": None, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "\n",
        "        print(\"Conv ret:\", outputs.shape)\n",
        "        # Residual connection\n",
        "        outputs += inputs\n",
        "\n",
        "        # Normalize\n",
        "        outputs = layer_normalization(outputs)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the neural model"
      ],
      "metadata": {
        "id": "4eZ1jx-THNpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "CURRENT_DIR = Path('.')\n",
        "UTILS_DIR = CURRENT_DIR / '../'\n",
        "sys.path.append(UTILS_DIR.absolute().as_posix())\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "def batched_gather1(tensor, indices):\n",
        "    \"\"\"Gather in batch from a tensor of arbitrary size.\n",
        "\n",
        "    In pseduocode this module will produce the following:\n",
        "    output[i] = tf.gather(tensor[i], indices[i])\n",
        "\n",
        "    Args:\n",
        "      tensor: Tensor of arbitrary size.\n",
        "      indices: Vector of indices.\n",
        "    Returns:\n",
        "      output: A tensor of gathered values.\n",
        "    \"\"\"\n",
        "    shape = (tensor.get_shape().as_list())\n",
        "    flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n",
        "    indices = tf.convert_to_tensor(indices)\n",
        "    offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)\n",
        "    offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)\n",
        "    output = tf.gather(flat_first, indices + offset)\n",
        "    return output\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "class AttentionClassifier(object):\n",
        "    def __init__(self, config):\n",
        "        self.max_len = config[\"max_len\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.n_class = config[\"n_class\"]\n",
        "        self.learning_rate = config[\"learning_rate\"]\n",
        "\n",
        "        # placeholder\n",
        "        self.x = tf.placeholder(tf.int32, [config[\"batch_size\"], self.max_len])\n",
        "\n",
        "        self.label = tf.placeholder(tf.float32, [config[\"batch_size\"], self.n_class], name='input_y')\n",
        "\n",
        "#        self.hot_label = tf.one_hot(self.label, self.n_class)\n",
        "        self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    def build_graph(self):\n",
        "        print(\"building graph...\")\n",
        "        # EMBEDDLINGLAYER\n",
        "        self.embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
        "                                     trainable=True)\n",
        "#        self.embed_matrix = self.embeddings_var.weights\n",
        "        # CONCEPT VECTORS\n",
        "        self.W_concept = tf.Variable(tf.random_uniform([self.n_class, self.embedding_size], -1.0, 1.0), name=\"W_concept\",trainable=True)\n",
        "\n",
        "        n, d = self.n_class, self.embedding_size\n",
        "        self.pos_encoding = positional_encoding(self.max_len, d)\n",
        "        self.batch_embedded = tf.nn.embedding_lookup(self.embeddings_var, self.x) + self.pos_encoding\n",
        "\n",
        "        # self.em = tf.stack(res_mat)\n",
        "        # AVERAGE EMBEDDING\n",
        "        self.em = tf.reduce_mean(self.batch_embedded,1, name=\"lassfsfsft\")\n",
        "        # multi-head attention\n",
        "        self.ma = multihead_attention(queries=tf.nn.embedding_lookup(self.embeddings_var, self.x), keys=tf.nn.embedding_lookup(self.embeddings_var, self.x))\n",
        "        # FFN(x) = LN(x + point-wisely NN(x))\n",
        "        # AVERAGE HIDDEN REPRESENTATION\n",
        "        self.outputs = feedforward(self.ma, [self.hidden_size, self.embedding_size])\n",
        "        self.outputs = tf.reduce_mean(self.outputs,1, name=\"lassfsfsft\")\n",
        "\n",
        "\n",
        "        logits = tf.layers.dense(self.outputs, units=self.n_class)\n",
        "        # CROSS ENTROPY\n",
        "        self.loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.label))\n",
        "        self.prediction = tf.argmax(tf.nn.softmax(logits), 1)\n",
        "        correct_predictions = tf.equal(self.prediction, tf.argmax(self.label, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "        # REWARD LOSS\n",
        "        self.reward_class = tf.cast((batched_gather1(self.label,tf.cast(self.prediction, tf.int32))), tf.float32)*0.9\n",
        "        # PREDICT CONCEPT VECTOR\n",
        "        self.concept_vect = tf.tanh(tf.nn.embedding_lookup(self.W_concept,self.prediction ))\n",
        "\n",
        "        # COSINE DISTANCE CONCEPT VECTOR  AND AVERAFE EMBEDDING\n",
        "        self.coss = tf.losses.cosine_distance(tf.nn.l2_normalize(self.concept_vect, 1), tf.nn.l2_normalize((self.em), 1), 1)*tf.expand_dims(self.reward_class,1)\n",
        "        self.cos_distance = tf.reduce_mean(tf.losses.cosine_distance(tf.nn.l2_normalize(self.concept_vect, 1), tf.nn.l2_normalize((self.em), 1), 1)*tf.expand_dims(self.reward_class,1))\n",
        "#         COSINE DISTANCE CONCEPTVECTOR AND\n",
        "        self.mse = tf.reduce_mean(tf.losses.cosine_distance(tf.nn.l2_normalize(self.concept_vect, 1), tf.nn.l2_normalize((self.outputs), 1), 1)*tf.expand_dims(self.reward_class,1))\n",
        "        q = self.W_concept\n",
        "        # PAIRWISE DISTANCE SIMILAIRTY/DISTANCE\n",
        "        self.pair=tf.reduce_sum(tf.reduce_sum((tf.expand_dims(q, 1)-tf.expand_dims(q, 0))**2,2))\n",
        "        # optimization\n",
        "        # loss_to_minimize = self.loss\n",
        "        self.cosorg = self.cos_distance\n",
        "        self.pairorg = self.pair\n",
        "        loss_to_minimize = self.loss+ 224.8*self.cos_distance   -0.8*self.pair + + 0.4*self.mse\n",
        "        self.target_loss = loss_to_minimize\n",
        "        tvars = tf.trainable_variables()\n",
        "        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
        "                                                       name='train_step')\n",
        "        print(\"graph built successfully!\")\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"max_len\": 50,\n",
        "    \"hidden_size\": 128,\n",
        "    \"vocab_size\": 10000,\n",
        "    \"embedding_size\": 128,\n",
        "    \"n_class\": 2,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 512,\n",
        "    \"train_epoch\": 20\n",
        "}\n",
        "classifier = AttentionClassifier(config)\n",
        "classifier.build_graph()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# dev_batch = (x_dev, y_dev)\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "#\n",
        "NUM_WORDS = 10000\n",
        "INDEX_FROM = 3\n",
        "SEQUENCE_LENGTH = 250\n",
        "maxlen = 50\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "y_test =  keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        " #\n",
        "NUM_WORDS = 10000\n",
        "word_to_id = keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding = 'post')\n",
        "x_test =  sequence.pad_sequences(x_test, maxlen=maxlen, padding = 'post')\n",
        "\n",
        "\n",
        "total_parameters = 0\n",
        "for variable in tf.trainable_variables():\n",
        "    # shape is an array of tf.Dimension\n",
        "    shape = variable.get_shape()\n",
        "    print(shape)\n",
        "    print(len(shape))\n",
        "    variable_parameters = 1\n",
        "    for dim in shape:\n",
        "        print(dim)\n",
        "        variable_parameters *= dim.value\n",
        "    print(variable_parameters)\n",
        "    total_parameters += variable_parameters\n",
        "print(total_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfG06Lw6DtSK",
        "outputId": "7db7efa1-91a7-4b0f-a714-c3307bd4a8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building graph...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b62e497a4ff8>:36: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)  # [N, T_q, C]\n",
            "<ipython-input-1-b62e497a4ff8>:37: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n",
            "<ipython-input-1-b62e497a4ff8>:38: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n",
            "<ipython-input-1-b62e497a4ff8>:79: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "<ipython-input-1-b62e497a4ff8>:103: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.layers.conv1d(**params)\n",
            "<ipython-input-1-b62e497a4ff8>:108: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.layers.conv1d(**params)\n",
            "<ipython-input-2-797824356c62>:108: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits = tf.layers.dense(self.outputs, units=self.n_class)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv ret: (512, 50, 128)\n",
            "graph built successfully!\n",
            "(10000, 128)\n",
            "2\n",
            "10000\n",
            "128\n",
            "1280000\n",
            "(2, 128)\n",
            "2\n",
            "2\n",
            "128\n",
            "256\n",
            "(128, 128)\n",
            "2\n",
            "128\n",
            "128\n",
            "16384\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128, 128)\n",
            "2\n",
            "128\n",
            "128\n",
            "16384\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128, 128)\n",
            "2\n",
            "128\n",
            "128\n",
            "16384\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(1, 128, 128)\n",
            "3\n",
            "1\n",
            "128\n",
            "128\n",
            "16384\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(1, 128, 128)\n",
            "3\n",
            "1\n",
            "128\n",
            "128\n",
            "16384\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128,)\n",
            "1\n",
            "128\n",
            "128\n",
            "(128, 2)\n",
            "2\n",
            "128\n",
            "2\n",
            "256\n",
            "(2,)\n",
            "1\n",
            "2\n",
            "2\n",
            "1363586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the training process: You will need to train it longer to get better result**"
      ],
      "metadata": {
        "id": "iTTXBX-sHShs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_acc (x_batch, y_batch, sess):\n",
        "    feed_dict = {classifier.x: x_batch,\n",
        "                 classifier.label: y_batch,\n",
        "                 classifier.keep_prob: 1.0}\n",
        "\n",
        "    accuracy= sess.run(\n",
        "        [classifier.accuracy],\n",
        "        feed_dict)\n",
        "    return accuracy[0]\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_acc (xxtr,yytr ):\n",
        "    acc_test = []\n",
        "    for index in range(int(xxtr.shape[0]/BATCH_SIZE)):\n",
        "        x_batch = xxtr[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "        y_batch = yytr[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "        dev_acc = calculate_acc(x_batch, y_batch, sess)\n",
        "        acc_test.append(dev_acc)\n",
        "\n",
        "    return np.mean(acc_test)\n",
        "\n",
        "def train_step(x_batch, y_batch, sess,epoch):\n",
        "    \"\"\"\n",
        "    A single training step\n",
        "    \"\"\"\n",
        "\n",
        "    feed_dict = {classifier.x: x_batch,\n",
        "                 classifier.label: y_batch,\n",
        "                 classifier.keep_prob: 0.50,\n",
        "                 }\n",
        "\n",
        "    step,_,  loss, accuracy, cos, pair = sess.run(\n",
        "        [classifier.global_step,classifier.train_op, classifier.target_loss, classifier.accuracy, classifier.cosorg, classifier.pairorg],\n",
        "        feed_dict)\n",
        "\n",
        "    return loss, accuracy, cos, pair\n",
        "\n",
        "for epoch in range(50):\n",
        "    loss_data = []\n",
        "    acc_train =[]\n",
        "    acc_dev = []\n",
        "    pairs, coss = [], []\n",
        "    indices = np.arange((x_train.shape[0]))\n",
        "    np.random.shuffle(indices)\n",
        "    x_train = x_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "    for index in range(int(x_train.shape[0]/BATCH_SIZE)):\n",
        "        x_batch = x_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "        y_batch = y_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "        lossd, accuracyd, cos, pair  = train_step(x_batch, y_batch,sess,epoch)\n",
        "        loss_data.append(lossd)\n",
        "        acc_train.append(accuracyd)\n",
        "        pairs.append(pair)\n",
        "        coss.append(cos)\n",
        "    print(f'Epoch number : {epoch} | Train accuracy : {np.mean(acc_train)} | Train loss {np.mean(loss_data)}')\n",
        "acc_test = evaluate_acc(x_test, y_test)\n",
        "\n",
        "\n",
        "print(\"Test accuracy: %.3f \" % acc_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l263bz6ZDygo",
        "outputId": "260db9ca-faba-44a2-acc9-3c53604837f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number : 0 | Train accuracy : 0.5989990234375 | Train loss -37.13450622558594\n",
            "Epoch number : 1 | Train accuracy : 0.6838786005973816 | Train loss -56.68708419799805\n",
            "Epoch number : 2 | Train accuracy : 0.7323405146598816 | Train loss -87.26639556884766\n",
            "Epoch number : 3 | Train accuracy : 0.7694905400276184 | Train loss -123.22183227539062\n",
            "Epoch number : 4 | Train accuracy : 0.7920328974723816 | Train loss -165.0445098876953\n",
            "Epoch number : 5 | Train accuracy : 0.8084716796875 | Train loss -211.3075408935547\n",
            "Epoch number : 6 | Train accuracy : 0.8197021484375 | Train loss -262.22784423828125\n",
            "Epoch number : 7 | Train accuracy : 0.8292236328125 | Train loss -317.2154541015625\n",
            "Epoch number : 8 | Train accuracy : 0.8384602665901184 | Train loss -376.0726013183594\n",
            "Epoch number : 9 | Train accuracy : 0.8455403447151184 | Train loss -439.064208984375\n",
            "Epoch number : 10 | Train accuracy : 0.8530680537223816 | Train loss -505.6951599121094\n",
            "Epoch number : 11 | Train accuracy : 0.8607991337776184 | Train loss -576.056396484375\n",
            "Epoch number : 12 | Train accuracy : 0.8693440556526184 | Train loss -650.051513671875\n",
            "Epoch number : 13 | Train accuracy : 0.874755859375 | Train loss -728.1589965820312\n",
            "Epoch number : 14 | Train accuracy : 0.8831787109375 | Train loss -809.6409301757812\n",
            "Epoch number : 15 | Train accuracy : 0.88818359375 | Train loss -895.2647094726562\n",
            "Epoch number : 16 | Train accuracy : 0.90087890625 | Train loss -983.47998046875\n",
            "Epoch number : 17 | Train accuracy : 0.907958984375 | Train loss -1076.2091064453125\n",
            "Epoch number : 18 | Train accuracy : 0.9128825068473816 | Train loss -1172.8629150390625\n",
            "Epoch number : 19 | Train accuracy : 0.9224039912223816 | Train loss -1272.644775390625\n",
            "Epoch number : 20 | Train accuracy : 0.9286295771598816 | Train loss -1376.5606689453125\n",
            "Epoch number : 21 | Train accuracy : 0.9305012822151184 | Train loss -1484.8603515625\n",
            "Epoch number : 22 | Train accuracy : 0.9432373046875 | Train loss -1595.23046875\n",
            "Epoch number : 23 | Train accuracy : 0.9468587040901184 | Train loss -1710.7867431640625\n",
            "Epoch number : 24 | Train accuracy : 0.9551594853401184 | Train loss -1829.310546875\n",
            "Epoch number : 25 | Train accuracy : 0.9593098759651184 | Train loss -1952.267578125\n",
            "Epoch number : 26 | Train accuracy : 0.9551188349723816 | Train loss -2080.243408203125\n",
            "Epoch number : 27 | Train accuracy : 0.9688720703125 | Train loss -2209.3232421875\n",
            "Epoch number : 28 | Train accuracy : 0.9735514521598816 | Train loss -2343.633056640625\n",
            "Epoch number : 29 | Train accuracy : 0.9754231572151184 | Train loss -2482.112548828125\n",
            "Epoch number : 30 | Train accuracy : 0.9779459834098816 | Train loss -2624.405029296875\n",
            "Epoch number : 31 | Train accuracy : 0.9784749150276184 | Train loss -2770.766357421875\n",
            "Epoch number : 32 | Train accuracy : 0.9830322265625 | Train loss -2920.410888671875\n",
            "Epoch number : 33 | Train accuracy : 0.9864094853401184 | Train loss -3073.975341796875\n",
            "Epoch number : 34 | Train accuracy : 0.9894205927848816 | Train loss -3231.529296875\n",
            "Epoch number : 35 | Train accuracy : 0.9899088740348816 | Train loss -3393.262939453125\n",
            "Epoch number : 36 | Train accuracy : 0.9903971552848816 | Train loss -3558.645263671875\n",
            "Epoch number : 37 | Train accuracy : 0.9878336787223816 | Train loss -3728.506591796875\n",
            "Epoch number : 38 | Train accuracy : 0.9900309443473816 | Train loss -3901.392578125\n",
            "Epoch number : 39 | Train accuracy : 0.9936930537223816 | Train loss -4077.950927734375\n",
            "Epoch number : 40 | Train accuracy : 0.9939778447151184 | Train loss -4258.8046875\n",
            "Epoch number : 41 | Train accuracy : 0.9961751103401184 | Train loss -4443.38525390625\n",
            "Epoch number : 42 | Train accuracy : 0.9965006709098816 | Train loss -4632.072265625\n",
            "Epoch number : 43 | Train accuracy : 0.9972737431526184 | Train loss -4824.67138671875\n",
            "Epoch number : 44 | Train accuracy : 0.9966227412223816 | Train loss -5021.48193359375\n",
            "Epoch number : 45 | Train accuracy : 0.9957275390625 | Train loss -5222.32861328125\n",
            "Epoch number : 46 | Train accuracy : 0.993896484375 | Train loss -5427.32177734375\n",
            "Epoch number : 47 | Train accuracy : 0.9937337040901184 | Train loss -5636.20263671875\n",
            "Epoch number : 48 | Train accuracy : 0.994140625 | Train loss -5849.04150390625\n",
            "Epoch number : 49 | Train accuracy : 0.9951578974723816 | Train loss -6065.96630859375\n",
            "Test accuracy: 0.751 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the performance of compressed model"
      ],
      "metadata": {
        "id": "G9TWEBaFHWUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import timeit\n",
        "import tqdm\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "acc_test = []\n",
        "grd_pred = []\n",
        "model_pred = []\n",
        "acc = 0\n",
        "total_samples = 0\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "acc = 0\n",
        "total_samples = 0\n",
        "model_pred = []\n",
        "grd_pred = []\n",
        "neur_pred = []\n",
        "f1_compressed = []\n",
        "\n",
        "for t in range(1):\n",
        "    indxstry = []\n",
        "    res_acc = []\n",
        "    rat_siz = []\n",
        "    target_n = 29\n",
        "    tot = 0\n",
        "\n",
        "    for index in range(int(x_test.shape[0]/BATCH_SIZE)):\n",
        "        x_batch = x_test[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "        y_batch = y_test[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "\n",
        "        feed_dict = {classifier.x: x_batch,\n",
        "                 classifier.label: y_batch,\n",
        "                 classifier.keep_prob: 1.0}\n",
        "        embds= sess.run(classifier.batch_embedded, feed_dict)\n",
        "\n",
        "        concepts = (sess.run(classifier.W_concept, feed_dict)  )\n",
        "        em = sess.run(classifier.em, feed_dict)\n",
        "\n",
        "        cospred = sess.run(classifier.prediction, feed_dict)\n",
        "        cur_batch = []\n",
        "        model_out = sess.run(classifier.prediction, feed_dict)\n",
        "        start = timeit.default_timer()\n",
        "        for j in (range (BATCH_SIZE)):\n",
        "            inp = em[j]\n",
        "            concepts = concepts\n",
        "            sim= []\n",
        "            for c in range (concepts.shape[0]):\n",
        "                xc = (concepts[c])\n",
        "                cos_sim = dot(inp, xc)/(norm(inp)*norm(xc))\n",
        "                sim.append(cos_sim)\n",
        "            preds1 = sim.index(max(sim))\n",
        "\n",
        "\n",
        "            preds = np.zeros(num_classes)\n",
        "\n",
        "            preds[sim.index(max(sim))] = 1\n",
        "            model_pred.append(preds)\n",
        "            grd_pred.append(y_batch[j])\n",
        "            md = np.zeros(num_classes)\n",
        "            a = model_out[j]\n",
        "            md[a] = 1\n",
        "            neur_pred.append(md)\n",
        "\n",
        "\n",
        "            if preds1 == list(y_batch[j]).index(1):\n",
        "                acc += 1\n",
        "            total_samples += 1\n",
        "\n",
        "acc = (acc/total_samples)*100\n",
        "print(\"Accuracy Compressed \", acc)\n",
        "print(\"F1 compressed \", f1_score(grd_pred, model_pred, average=\"macro\"))\n",
        "print(\"Precision compressed \", precision_score(grd_pred, model_pred, average=\"macro\"))\n",
        "print(\"Recall compressed \", recall_score(grd_pred, model_pred, average=\"macro\"))\n",
        "print(\"F1 org \", f1_score(grd_pred, neur_pred, average=\"macro\"))\n",
        "print(\"Precision org \", precision_score(grd_pred, neur_pred, average=\"macro\"))\n",
        "print(\"Recall org \", recall_score(grd_pred, neur_pred, average=\"macro\"))\n",
        "print(classification_report(grd_pred, model_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE04CZODEb-M",
        "outputId": "ba507c9f-162d-4837-874d-6ecaf4953b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Compressed  78.12906901041666\n",
            "F1 compressed  0.7812869961206241\n",
            "Precision compressed  0.7813215220133478\n",
            "Recall compressed  0.7812968710514968\n",
            "F1 org  0.750661195019874\n",
            "Precision org  0.7514911991200398\n",
            "Recall org  0.7508451206164369\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.79      0.78     12273\n",
            "           1       0.78      0.78      0.78     12303\n",
            "\n",
            "   micro avg       0.78      0.78      0.78     24576\n",
            "   macro avg       0.78      0.78      0.78     24576\n",
            "weighted avg       0.78      0.78      0.78     24576\n",
            " samples avg       0.78      0.78      0.78     24576\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature attribution**"
      ],
      "metadata": {
        "id": "NcPEjHJiV9vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_review = \"a very good movie\".lower()\n",
        "tokenized_input = input_review.split()\n",
        "tokenized_input = [word_to_id[x] for x in tokenized_input]\n",
        "tokenized_input = sequence.pad_sequences([tokenized_input], maxlen=maxlen, padding = 'post')\n",
        "repeated_list = [tokenized_input for _ in range(BATCH_SIZE)]\n",
        "repeated_list = np.array(repeated_list)[:,0,:]\n",
        "feed_dict = {classifier.x: repeated_list,\n",
        "          classifier.label: y_batch,\n",
        "          classifier.keep_prob: 1.0}\n",
        "embds= sess.run(classifier.batch_embedded, feed_dict) [0] #[1,d]\n",
        "concepts = (sess.run(classifier.W_concept, feed_dict)  )  # [num_classes, d]\n",
        "mean_emb = np.mean(embds,0) #[1,dim]\n",
        "print(f'Mean emb shape : {mean_emb.shape} | concepts shape : {concepts.shape}')\n",
        "#prediction using the compressed model\n",
        "classes_target = ['Negative sentiment', 'Positive sentiment']\n",
        "sim= []\n",
        "for c in range (concepts.shape[0]):\n",
        "    xc = (concepts[c])\n",
        "    cos_sim = dot(mean_emb, xc)/(norm(mean_emb)*norm(xc))\n",
        "    sim.append(cos_sim)\n",
        "pred_class_index = sim.index(max(sim))\n",
        "print(f'Predicted sentiment : {classes_target[pred_class_index]}')\n",
        "# calculate feature attribution\n",
        "dict_feature_attr = {}\n",
        "for x in range(len(input_review.split())):\n",
        "  cos_sim = dot(embds[x], concepts[pred_class_index])/(norm(embds[x])*norm(concepts[pred_class_index]))\n",
        "  dict_feature_attr[input_review.split()[x]] = cos_sim\n",
        "print(f'Feature attributions : {dict_feature_attr}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHdPUz3MGXsQ",
        "outputId": "3547b576-2120-41c5-9a5e-24a29c2a51d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean emb shape : (128,) | concepts shape : (2, 128)\n",
            "Predicted sentiment : Positive sentiment\n",
            "Feature attributions : {'a': 0.2234983, 'very': 0.6901409, 'good': 0.32225192, 'movie': -0.39585334}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYvr1Vr0WhiK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}